{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec28d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# Load an image for prediction (for example, from a file)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import timm\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b19fc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98505 files belonging to 3 classes.\n",
      "Found 21108 files belonging to 3 classes.\n",
      "Found 21111 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)\n",
    "DATASET_PATH = \"../../Datasets/LFR Dataset/split\"\n",
    "\n",
    "# Load datasets\n",
    "train_ds = image_dataset_from_directory(\n",
    "    os.path.join(DATASET_PATH, \"train\"),\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    os.path.join(DATASET_PATH, \"val\"),\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    os.path.join(DATASET_PATH, \"test\"),\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "# # Enable prefetching for performance\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "# train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "# test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "# # Check how many samples are in each dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3e8af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 corrupt images\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def remove_corrupt_images(folder_path):\n",
    "    removed = 0\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            file_path = os.path.join(root, f)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()  # Just verifies, doesn't load entire image\n",
    "            except Exception as e:\n",
    "                print(f\"Removing corrupt image: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "                removed += 1\n",
    "    print(f\"Removed {removed} corrupt images\")\n",
    "\n",
    "# Run on your dataset\n",
    "remove_corrupt_images(\"../../Datasets/LFR Dataset/split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518c8250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3079\n",
      "Test dataset size: 660\n",
      "val dataset size: 660\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_ds)}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "print(f\"val dataset size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e281b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf  # Also make sure this is imported\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomBrightness(0.1),\n",
    "    layers.GaussianNoise(0.02),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b910a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "\n",
    "base_model = EfficientNetB0(input_shape=IMG_SIZE + (3,),\n",
    "                            include_top=False,\n",
    "                            weights='imagenet')\n",
    "base_model.trainable = False  # Freeze base\n",
    "\n",
    "inputs = Input(shape=IMG_SIZE + (3,))\n",
    "x = data_augmentation(inputs)\n",
    "x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(3, activation='softmax')(x)  # 3 classes\n",
    "\n",
    "model = Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee0863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',  # Optimizer of your choice\n",
    "    loss='sparse_categorical_crossentropy',  # Since you're using integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2fac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 193ms/step - accuracy: 0.6986 - loss: 0.6796 - val_accuracy: 0.7330 - val_loss: 0.6019\n",
      "Epoch 2/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 192ms/step - accuracy: 0.7189 - loss: 0.6254 - val_accuracy: 0.7296 - val_loss: 0.5957\n",
      "Epoch 3/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 184ms/step - accuracy: 0.7212 - loss: 0.6226 - val_accuracy: 0.7291 - val_loss: 0.6013\n",
      "Epoch 4/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 184ms/step - accuracy: 0.7215 - loss: 0.6231 - val_accuracy: 0.7315 - val_loss: 0.5954\n",
      "Epoch 5/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 188ms/step - accuracy: 0.7218 - loss: 0.6219 - val_accuracy: 0.7339 - val_loss: 0.5977\n",
      "Epoch 6/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 193ms/step - accuracy: 0.7197 - loss: 0.6254 - val_accuracy: 0.7335 - val_loss: 0.5894\n",
      "Epoch 7/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 191ms/step - accuracy: 0.7200 - loss: 0.6228 - val_accuracy: 0.7318 - val_loss: 0.6013\n",
      "Epoch 8/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 192ms/step - accuracy: 0.7195 - loss: 0.6237 - val_accuracy: 0.7376 - val_loss: 0.5832\n",
      "Epoch 9/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 190ms/step - accuracy: 0.7209 - loss: 0.6245 - val_accuracy: 0.7328 - val_loss: 0.5970\n",
      "Epoch 10/10\n",
      "\u001b[1m3079/3079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 188ms/step - accuracy: 0.7193 - loss: 0.6210 - val_accuracy: 0.7369 - val_loss: 0.5817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,  # Start with just one epoch for testing\n",
    "    validation_data=val_ds,\n",
    "    verbose=1  # Make sure you're able to see logs for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "665e067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../../models/face_dir_further.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"face_direction_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240ecba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd275ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
