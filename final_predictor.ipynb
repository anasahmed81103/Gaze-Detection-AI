{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f22f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# Load an image for prediction (for example, from a file)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57891a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# === Load models ===\n",
    "face_model = load_model(\"../models/face_direction_model2.h5\")\n",
    "eye_model = load_model(\"../models/eyes_dir_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a9ae8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Class labels ===\n",
    "face_labels = ['left', 'right', 'front']\n",
    "eye_labels = [\n",
    "    \"BottomCenter\",\n",
    "    \"BottomLeft\",\n",
    "    \"BottomRight\",\n",
    "    \"MiddleLeft\",\n",
    "    \"MiddleRight\",\n",
    "    \"TopCenter\",\n",
    "    \"TopLeft\",\n",
    "    \"TopRight\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9507ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Face Direction Model Prediction Scores ===\n",
      "left: 0.2462\n",
      "right: 0.4121\n",
      "front: 0.3418\n",
      "\n",
      "→ Final Direction: LEFT\n",
      "\n",
      "=== Eye Model Decision ===\n",
      "Left Eye: MiddleRight (0.4245)\n",
      "Right Eye: MiddleRight (0.5853)\n",
      "→ Synchronized Eye Label: MiddleRight\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Parameters ===\n",
    "img_path = \"../Datasets/t2.jpg\"  # Use the same image for both models\n",
    "face_img = cv2.imread(img_path)\n",
    "if face_img is None:\n",
    "    raise FileNotFoundError(f\"Image not found at path: {img_path}\")\n",
    "eye_img = face_img.copy()\n",
    "\n",
    "face_img_size = (128, 128)\n",
    "eye_img_size = (64, 64)\n",
    "\n",
    "# === Face direction prediction ===\n",
    "face_img_resized = cv2.resize(face_img, face_img_size)\n",
    "face_img_resized = face_img_resized.astype('float32') / 255.0\n",
    "face_img_array = np.expand_dims(face_img_resized, axis=0)\n",
    "\n",
    "face_prediction = face_model.predict(face_img_array, verbose=0)[0]\n",
    "left, right, front = face_prediction\n",
    "\n",
    "# === Custom logic for face direction ===\n",
    "diff_front_right = front - right\n",
    "\n",
    "\n",
    "if (\n",
    "    # If front is highest, but right is close enough, prefer right\n",
    "    (front >= 0.25 and 0.01 <= diff_front_right <= 0.18)\n",
    "    or\n",
    "    # If right is clearly stronger than left\n",
    "    (right > 0.5 and left < 0.25)\n",
    "    or\n",
    "    (right > 2 * left)\n",
    "):\n",
    "    predicted_face_dir = \"right\"\n",
    "elif (left > right) or (left > 0.2 and right < 0.5):\n",
    "    predicted_face_dir = \"left\"\n",
    "else:\n",
    "    predicted_face_dir = face_labels[np.argmax(face_prediction)]  # default to max\n",
    "\n",
    "\n",
    "# === Eye detection and prediction ===\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "rgb_eye_img = cv2.cvtColor(eye_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = face_mesh.process(rgb_eye_img)\n",
    "# === Fallback setup for Haar cascades ===\n",
    "eye_detected = False\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "\n",
    "h, w = eye_img.shape[:2]\n",
    "left_eye_prediction = None\n",
    "right_eye_prediction = None\n",
    "\n",
    "\n",
    "def preprocess_eye(eye_roi):\n",
    "    eye_gray = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2GRAY)\n",
    "    eye_resized = cv2.resize(eye_gray, eye_img_size)\n",
    "    eye_array = eye_resized.astype(\"float32\") / 255.0\n",
    "    eye_array = np.expand_dims(eye_array, axis=[0, -1])\n",
    "    return eye_array\n",
    "\n",
    "\n",
    "def predict_eye(eye_array):\n",
    "    prediction = eye_model.predict(eye_array, verbose=0)[0]\n",
    "    idx = np.argmax(prediction)\n",
    "    return eye_labels[idx], prediction[idx], prediction\n",
    "\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        left_eye_idx = [33, 133]\n",
    "        right_eye_idx = [362, 263]\n",
    "\n",
    "        def extract_eye(indices):\n",
    "            x = [int(face_landmarks.landmark[i].x * w) for i in indices]\n",
    "            y = [int(face_landmarks.landmark[i].y * h) for i in indices]\n",
    "            x1, x2 = min(x), max(x)\n",
    "            y1, y2 = min(y), max(y)\n",
    "            mx, my = int((x2 - x1) * 0.4), int((y2 - y1) * 1.2)\n",
    "            return max(x1 - mx, 0), max(y1 - my, 0), min(x2 + mx, w), min(y2 + my, h)\n",
    "\n",
    "        for label, indices in zip([\"Left\", \"Right\"], [left_eye_idx, right_eye_idx]):\n",
    "            x1, y1, x2, y2 = extract_eye(indices)\n",
    "            eye_roi = eye_img[y1:y2, x1:x2]\n",
    "            if eye_roi.size == 0:\n",
    "                continue\n",
    "            eye_array = preprocess_eye(eye_roi)\n",
    "            pred_label, prob, pred_vector = predict_eye(eye_array)\n",
    "            cv2.rectangle(eye_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(eye_img, f\"{label} Eye: {pred_label}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "            if label == \"Left\":\n",
    "                left_eye_prediction = (pred_label, prob, pred_vector)\n",
    "            else:\n",
    "                right_eye_prediction = (pred_label, prob, pred_vector)\n",
    "            eye_detected = True\n",
    "\n",
    "else:\n",
    "    # === Fallback: Haar Cascade eye detection ===\n",
    "    print(\"⚠️ MediaPipe failed, using Haar Cascade fallback.\")\n",
    "\n",
    "    gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w_, h_) in faces:\n",
    "        roi_gray = gray[y:y+h_, x:x+w_]\n",
    "        roi_color = eye_img[y:y+h_, x:x+w_]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        for i, (ex, ey, ew, eh) in enumerate(eyes[:2]):  # Limit to 2 eyes\n",
    "            if ew < 10 or eh < 10 or ew > w_ // 2:\n",
    "                continue\n",
    "\n",
    "            eye_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "            if eye_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            eye_array = preprocess_eye(eye_roi)\n",
    "            pred_label, prob, pred_vector = predict_eye(eye_array)\n",
    "\n",
    "            label = \"Left\" if i == 0 else \"Right\"\n",
    "            cv2.putText(eye_img, f\"{label} Eye: {pred_label}\", (x + ex, y + ey - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 255), 2)\n",
    "\n",
    "            if label == \"Left\":\n",
    "                left_eye_prediction = (pred_label, prob, pred_vector)\n",
    "            else:\n",
    "                right_eye_prediction = (pred_label, prob, pred_vector)\n",
    "            eye_detected = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Final unified logic (revised) ===\n",
    "# === Final unified logic (final adjusted version) ===\n",
    "eye_final_label = None\n",
    "\n",
    "if left_eye_prediction and right_eye_prediction:\n",
    "    left_label = left_eye_prediction[0]\n",
    "    right_label = right_eye_prediction[0]\n",
    "\n",
    "    if left_label == right_label:\n",
    "        eye_final_label = left_label\n",
    "    else:\n",
    "        # You can decide which one is more confident\n",
    "        if left_eye_prediction[1] > right_eye_prediction[1]:\n",
    "            eye_final_label = left_label\n",
    "        else:\n",
    "            eye_final_label = right_label\n",
    "# === Final unified logic ===\n",
    "final_direction = predicted_face_dir\n",
    "\n",
    "if left_eye_prediction and right_eye_prediction:\n",
    "    # Get the prediction vectors for both eyes\n",
    "    left_vector = left_eye_prediction[2]\n",
    "    right_vector = right_eye_prediction[2]\n",
    "\n",
    "    # Define individual thresholds\n",
    "    thresholds = {\n",
    "        \"TopCenter\": 0.70,\n",
    "        \"BottomCenter\": 0.98  # Less influence\n",
    "    }\n",
    "\n",
    "    for label, threshold in thresholds.items():\n",
    "        idx = eye_labels.index(label)\n",
    "        if left_vector[idx] > threshold or right_vector[idx] > threshold:\n",
    "            final_direction = \"front\"\n",
    "            break\n",
    "\n",
    "# === Display results ===\n",
    "cv2.putText(eye_img, f\"Final Direction: {final_direction.upper()}\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow(\"Unified Gaze Output\", eye_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# === Console logs ===\n",
    "print(\"\\n=== Face Direction Model Prediction Scores ===\")\n",
    "for i, label in enumerate(face_labels):\n",
    "    print(f\"{label}: {face_prediction[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n→ Final Direction: {final_direction.upper()}\")\n",
    "\n",
    "if eye_final_label:\n",
    "    print(\"\\n=== Eye Model Decision ===\")\n",
    "    print(f\"Left Eye: {left_eye_prediction[0]} ({left_eye_prediction[1]:.4f})\")\n",
    "    print(f\"Right Eye: {right_eye_prediction[0]} ({right_eye_prediction[1]:.4f})\")\n",
    "    print(f\"→ Synchronized Eye Label: {eye_final_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "67d45fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Left Eye] Left: BottomCenter (0.32)\n",
      "Probabilities:\n",
      "  BottomCenter: 0.3165\n",
      "  BottomLeft: 0.2133\n",
      "  BottomRight: 0.2345\n",
      "  MiddleLeft: 0.0846\n",
      "  MiddleRight: 0.0952\n",
      "  TopCenter: 0.0233\n",
      "  TopLeft: 0.0249\n",
      "  TopRight: 0.0076\n",
      "[Right Eye] Right: TopLeft (0.69)\n",
      "Probabilities:\n",
      "  BottomCenter: 0.0008\n",
      "  BottomLeft: 0.0009\n",
      "  BottomRight: 0.0014\n",
      "  MiddleLeft: 0.0775\n",
      "  MiddleRight: 0.0706\n",
      "  TopCenter: 0.0295\n",
      "  TopLeft: 0.6907\n",
      "  TopRight: 0.1286\n",
      "\n",
      "=== Face Direction Model Prediction Scores ===\n",
      "left: 0.2695\n",
      "right: 0.3532\n",
      "front: 0.3773\n",
      "\n",
      "→ Final Direction: RIGHT\n",
      "\n",
      "=== Eye Model Decision ===\n",
      "Left Eye: BottomCenter (0.3165)\n",
      "Right Eye: TopLeft (0.6907)\n",
      "→ Synchronized Eye Label: TopLeft\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Parameters ===\n",
    "img_path = \"../Datasets/t5.jpg\"  # Use the same image for both models\n",
    "face_img = cv2.imread(img_path)\n",
    "if face_img is None:\n",
    "    raise FileNotFoundError(f\"Image not found at path: {img_path}\")\n",
    "eye_img = face_img.copy()\n",
    "\n",
    "face_img_size = (128, 128)\n",
    "eye_img_size = (64, 64)\n",
    "\n",
    "# === Face direction prediction ===\n",
    "face_img_resized = cv2.resize(face_img, face_img_size)\n",
    "face_img_resized = face_img_resized.astype('float32') / 255.0\n",
    "face_img_array = np.expand_dims(face_img_resized, axis=0)\n",
    "\n",
    "face_prediction = face_model.predict(face_img_array, verbose=0)[0]\n",
    "left, right, front = face_prediction\n",
    "\n",
    "# === Custom logic for face direction ===\n",
    "diff_front_right = front - right\n",
    "\n",
    "\n",
    "if (\n",
    "    # If front is highest, but right is close enough, prefer right\n",
    "    (front >= 0.25 and 0.01 <= diff_front_right <= 0.2)\n",
    "    or\n",
    "    # If right is clearly stronger than left\n",
    "    (right > 0.5 and left < 0.25)\n",
    "    or\n",
    "    (right > 2 * left)\n",
    "):\n",
    "    predicted_face_dir = \"right\"\n",
    "elif (left > right) or (left > 0.2 and right < 0.5):\n",
    "    predicted_face_dir = \"left\"\n",
    "else:\n",
    "    predicted_face_dir = face_labels[np.argmax(face_prediction)]  # default to max\n",
    "\n",
    "\n",
    "\n",
    "# === Read input image ===\n",
    "img = eye_img  # <-- Change as needed\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not found!\")\n",
    "\n",
    "\n",
    "# === Load trained model ===\n",
    "model = eye_model\n",
    "img_size = eye_img_size\n",
    "\n",
    "# === Class labels ===\n",
    "class_labels = eye_labels  # <-- Dynamically picked\n",
    "\n",
    "# === Initialize MediaPipe face mesh ===\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "# === Load Haar cascades (fallback if MediaPipe fails) ===\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# Resize if too large\n",
    "max_width, max_height = 800, 600\n",
    "h, w = img.shape[:2]\n",
    "if w > max_width or h > max_height:\n",
    "    scale = min(max_width / w, max_height / h)\n",
    "    img = cv2.resize(img, (int(w * scale), int(h * scale)))\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "# Convert to RGB\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "results = face_mesh.process(rgb_img)\n",
    "\n",
    "# Flag to check if MediaPipe worked\n",
    "eye_detected = False\n",
    "\n",
    "# === Helper functions ===\n",
    "def preprocess_eye(eye_roi):\n",
    "    eye_gray = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2GRAY)\n",
    "    eye_resized = cv2.resize(eye_gray, img_size)\n",
    "    eye_array = eye_resized.astype(\"float32\") / 255.0\n",
    "    eye_array = np.expand_dims(eye_array, axis=[0, -1])  # (1, 64, 64, 1)\n",
    "    return eye_array\n",
    "\n",
    "def predict_eye(eye_array):\n",
    "    prediction = model.predict(eye_array, verbose=0)[0]\n",
    "    predicted_idx = np.argmax(prediction)\n",
    "    return class_labels[predicted_idx], prediction[predicted_idx], prediction\n",
    "\n",
    "def print_probs(prediction, class_labels):\n",
    "    print(\"Probabilities:\")\n",
    "    for label, prob in zip(class_labels, prediction):\n",
    "        print(f\"  {label}: {prob:.4f}\")\n",
    "    predicted_idx = np.argmax(prediction)\n",
    "    return class_labels[predicted_idx], prediction[predicted_idx]\n",
    "\n",
    "# === Try MediaPipe face mesh first ===\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        left_eye_indices = [33, 133]\n",
    "        right_eye_indices = [362, 263]\n",
    "\n",
    "        def extract_eye_region(indices):\n",
    "            x_coords = [int(face_landmarks.landmark[i].x * w) for i in indices]\n",
    "            y_coords = [int(face_landmarks.landmark[i].y * h) for i in indices]\n",
    "            x_min, x_max = min(x_coords), max(x_coords)\n",
    "            y_min, y_max = min(y_coords), max(y_coords)\n",
    "            margin_x = int((x_max - x_min) * 0.4)\n",
    "            margin_y = int((y_max - y_min) * 1.2)\n",
    "            x1 = max(x_min - margin_x, 0)\n",
    "            y1 = max(y_min - margin_y, 0)\n",
    "            x2 = min(x_max + margin_x, w)\n",
    "            y2 = min(y_max + margin_y, h)\n",
    "            return x1, y1, x2, y2\n",
    "\n",
    "        left_pred = right_pred = None\n",
    "\n",
    "        for eye_label, indices in zip([\"Left\", \"Right\"], [left_eye_indices, right_eye_indices]):\n",
    "            x1, y1, x2, y2 = extract_eye_region(indices)\n",
    "            eye_roi = img[y1:y2, x1:x2]\n",
    "            if eye_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            eye_array = preprocess_eye(eye_roi)\n",
    "            predicted_label, predicted_prob, prediction = predict_eye(eye_array)\n",
    "\n",
    "            if eye_label == \"Left\":\n",
    "                left_pred = (predicted_label, predicted_prob, prediction)\n",
    "            else:\n",
    "                right_pred = (predicted_label, predicted_prob, prediction)\n",
    "\n",
    "            label_text = f\"{eye_label}: {predicted_label} ({predicted_prob:.2f})\"\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.rectangle(eye_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(img, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            print(f\"[{eye_label} Eye] {label_text}\")\n",
    "            print_probs(prediction, class_labels)\n",
    "            eye_detected = True\n",
    "\n",
    "        if left_pred and right_pred:\n",
    "            # After MediaPipe (inside if left_pred and right_pred:) \n",
    "            left_eye_prediction = left_pred\n",
    "            right_eye_prediction = right_pred\n",
    "\n",
    "            \n",
    "\n",
    "           \n",
    "\n",
    "# === Fallback to Haar cascades ===\n",
    "if not eye_detected:\n",
    "    print(\"⚠️ MediaPipe failed, using Haar cascades...\")\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i, (ex, ey, ew, eh) in enumerate(eyes[:2]):\n",
    "            if ew < 10 or eh < 10 or ey > h // 2:\n",
    "                continue\n",
    "\n",
    "            eye_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "            if eye_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            eye_array = preprocess_eye(eye_roi)\n",
    "            predicted_label, predicted_prob, prediction = predict_eye(eye_array)\n",
    "            predictions.append((predicted_label, predicted_prob, prediction))\n",
    "\n",
    "            label_text = f\"Eye{i+1}: {predicted_label} ({predicted_prob:.2f})\"\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (255, 0, 0), 2)\n",
    "            cv2.rectangle(eye_img[y:y+h, x:x+w], (ex, ey), (ex+ew, ey+eh), (255, 0, 0), 2)\n",
    "\n",
    "            cv2.putText(roi_color, label_text, (ex, ey - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 0, 0), 1)\n",
    "\n",
    "            print(f\"[Haar Eye {i+1}] {label_text}\")\n",
    "            print_probs(prediction, class_labels)\n",
    "\n",
    "        if len(predictions) == 2:\n",
    "            # After Haar (inside if len(predictions) == 2:)\n",
    "            left_eye_prediction = predictions[0]\n",
    "            right_eye_prediction = predictions[1]\n",
    "\n",
    "            if predictions[0][1] > predictions[1][1]:\n",
    "                synchronized_label = predictions[0][0]\n",
    "            else:\n",
    "                synchronized_label = predictions[1][0]\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "eye_final_label = None\n",
    "\n",
    "if left_eye_prediction and right_eye_prediction:\n",
    "    left_label = left_eye_prediction[0]\n",
    "    right_label = right_eye_prediction[0]\n",
    "\n",
    "    if left_label == right_label:\n",
    "        eye_final_label = left_label\n",
    "    else:\n",
    "        # You can decide which one is more confident\n",
    "        if left_eye_prediction[1] > right_eye_prediction[1]:\n",
    "            eye_final_label = left_label\n",
    "        else:\n",
    "            eye_final_label = right_label\n",
    "\n",
    "# === Final unified logic ===\n",
    "final_direction = predicted_face_dir\n",
    "\n",
    "if left_eye_prediction and right_eye_prediction:\n",
    "    # Get the prediction vectors for both eyes\n",
    "    left_vector = left_eye_prediction[2]\n",
    "    right_vector = right_eye_prediction[2]\n",
    "\n",
    "    # Define individual thresholds\n",
    "    thresholds = {\n",
    "        \"TopCenter\": 0.70,\n",
    "        \"BottomCenter\": 0.98  # Less influence\n",
    "    }\n",
    "\n",
    "    for label, threshold in thresholds.items():\n",
    "        idx = eye_labels.index(label)\n",
    "        if left_vector[idx] > threshold or right_vector[idx] > threshold:\n",
    "            final_direction = \"front\"\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "if predicted_face_dir == \"right\":\n",
    "    if left_eye_prediction[0] == \"BottomCenter\":\n",
    "        prob_diff = left_eye_prediction[1] - right_eye_prediction[1]\n",
    "        if prob_diff > 0:\n",
    "            final_direction = \"front\"\n",
    "\n",
    "    elif right_eye_prediction[0] == \"BottomCenter\":\n",
    "        prob_diff = right_eye_prediction[1] - left_eye_prediction[1]\n",
    "        if prob_diff > 0:\n",
    "            final_direction = \"front\"\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "if left_eye_prediction[1] > right_eye_prediction[1]:\n",
    "    synchronized_label = left_pred[0]\n",
    "else:\n",
    "    synchronized_label = right_pred[0]\n",
    "\n",
    "# cv2.putText(img, f\"Synchronized: {synchronized_label}\", (20, 30),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "#                         print(f\"=> Final Syn chronized Label: {synchronized_label}\\n\")\n",
    "\n",
    "# === Display results ===\n",
    "cv2.putText(img, f\"Final Direction: {final_direction.upper()}\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow(\"Unified Gaze Output\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# === Console logs ===\n",
    "print(\"\\n=== Face Direction Model Prediction Scores ===\")\n",
    "for i, label in enumerate(face_labels):\n",
    "    print(f\"{label}: {face_prediction[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n→ Final Direction: {final_direction.upper()}\")\n",
    "\n",
    "if eye_final_label:\n",
    "    print(\"\\n=== Eye Model Decision ===\")\n",
    "    print(f\"Left Eye: {left_eye_prediction[0]} ({left_eye_prediction[1]:.4f})\")\n",
    "    print(f\"Right Eye: {right_eye_prediction[0]} ({right_eye_prediction[1]:.4f})\")\n",
    "    print(f\"→ Synchronized Eye Label: {eye_final_label}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
