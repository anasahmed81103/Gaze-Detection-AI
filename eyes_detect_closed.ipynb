{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677e6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# Load an image for prediction (for example, from a file)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62177561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4103 images belonging to 2 classes.\n",
      "Found 1704 images belonging to 2 classes.\n",
      "Found 4232 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_height, img_width = 64, 64\n",
    "batch_size = 32\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_path = '../Datasets/Eyes_Data/TrainingSet'\n",
    "test_path = '../Datasets/Eyes_Data/TestSet'\n",
    "cv_path = '../Datasets/Eyes_Data/CVSet'\n",
    "\n",
    "\n",
    "train_datagen  = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    shear_range=0.2,  # Random shear transformation\n",
    "    zoom_range=0.2,  # Random zoom transformation\n",
    "    horizontal_flip=True  # Random horizontal flip\n",
    ")\n",
    "\n",
    "\n",
    "cv_datagen  = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    shear_range=0.2,  # Random shear transformation\n",
    "    zoom_range=0.2,  # Random zoom transformation\n",
    "    horizontal_flip=True  # Random horizontal flip\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_gen = cv_datagen.flow_from_directory(\n",
    "    cv_path,  # Using the new validation set\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False  # So you can match predictions to filenames\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d3d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m589,952\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">682,753</span> (2.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m682,753\u001b[0m (2.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">682,753</span> (2.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m682,753\u001b[0m (2.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff73306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d36e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 952ms/step - accuracy: 0.5497 - loss: 0.6844 - val_accuracy: 0.8034 - val_loss: 0.4996\n",
      "Epoch 2/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.8511 - loss: 0.3838 - val_accuracy: 0.8369 - val_loss: 0.3987\n",
      "Epoch 3/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - accuracy: 0.8876 - loss: 0.2989 - val_accuracy: 0.8586 - val_loss: 0.3569\n",
      "Epoch 4/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - accuracy: 0.8976 - loss: 0.2550 - val_accuracy: 0.8985 - val_loss: 0.2642\n",
      "Epoch 5/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - accuracy: 0.9132 - loss: 0.2409 - val_accuracy: 0.9108 - val_loss: 0.2477\n",
      "Epoch 6/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 134ms/step - accuracy: 0.9139 - loss: 0.2198 - val_accuracy: 0.8996 - val_loss: 0.2612\n",
      "Epoch 7/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 309ms/step - accuracy: 0.9203 - loss: 0.2136 - val_accuracy: 0.8914 - val_loss: 0.2800\n",
      "Epoch 8/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - accuracy: 0.9228 - loss: 0.2052 - val_accuracy: 0.9131 - val_loss: 0.2429\n",
      "Epoch 9/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.9315 - loss: 0.1763 - val_accuracy: 0.8967 - val_loss: 0.2639\n",
      "Epoch 10/10\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.9395 - loss: 0.1703 - val_accuracy: 0.9026 - val_loss: 0.2496\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,\n",
    "    validation_data=val_gen,  # <- CV set used here\n",
    "    callbacks=[],  # (Optional) Add EarlyStopping or ReduceLROnPlateau if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ec59a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 238ms/step - accuracy: 0.7426 - loss: 0.5867\n",
      "Test Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_gen)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbf6824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../Datasets/cc.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Prediction: 0.4459 — 👁️ Closed Eyes\n",
      "Processing: ../Datasets/oo.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Prediction: 0.8774 — 👁️ Open Eyes\n",
      "Processing: ../Datasets/oo2.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Prediction: 0.0003 — 👁️ Closed Eyes\n"
     ]
    }
   ],
   "source": [
    "img_path = [\"../Datasets/cc.jpg\" , \"../Datasets/oo.jpg\" , \"../Datasets/oo2.jpg\"]  # Replace with the image you want to classify\n",
    "\n",
    "\n",
    "for path in img_path:\n",
    "    print(f\"Processing: {path}\")\n",
    "    \n",
    "    # Load image in grayscale\n",
    "    img = load_img(path, color_mode=\"grayscale\", target_size=(img_height, img_width))\n",
    "    img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Reshape to (1, 64, 64, 1)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    if prediction < 0.5:\n",
    "        print(f\"Prediction: {prediction[0][0]:.4f} — 👁️ Closed Eyes\")\n",
    "    else:\n",
    "        print(f\"Prediction: {prediction[0][0]:.4f} — 👁️ Open Eyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb99eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#model.save(\"../models/open_close_eyes_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b598b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Load your trained model ===\n",
    "#model = load_model(\"../models/open_close_eyes_model.h5\")  # Replace with actual path\n",
    "img_size = (64, 64)  # Model input size\n",
    "\n",
    "# === Load Haar cascades ===\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# === Read image ===\n",
    "img = cv2.imread(\"../Datasets/d3.jpg\")  # Replace with your image path\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# === Detect faces ===\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "    # === Detect eyes in face ROI ===\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex, ey, ew, eh) in eyes:\n",
    "        # Eye region must be in upper half of face\n",
    "        if ey > h // 2:\n",
    "            continue\n",
    "\n",
    "        # Optional: filter eyes based on size\n",
    "        if ew < 10 or eh < 10 or ew > w // 2:\n",
    "            continue\n",
    "        eye_gray = roi_gray[ey:ey+eh, ex:ex+ew]\n",
    "        eye_resized = cv2.resize(eye_gray, img_size)  # Resize to 64x64\n",
    "        eye_array = eye_resized.astype(\"float32\") / 255.0  # Normalize\n",
    "        eye_array = np.expand_dims(eye_array, axis=-1)  # Add channel dim\n",
    "        eye_array = np.expand_dims(eye_array, axis=0)   # Add batch dim\n",
    "\n",
    "        # === Predict ===\n",
    "        prediction = model.predict(eye_array)[0][0]\n",
    "        label = \"Open\" if prediction >= 0.5 else \"Closed\"\n",
    "\n",
    "        # === Draw results ===\n",
    "        cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "        cv2.putText(roi_color, label, (ex, ey - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6, (255, 255, 255), 2)\n",
    "\n",
    "# === Display final result ===\n",
    "cv2.imshow(\"Eye Detection & Classification\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "870f2d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65340 images belonging to 2 classes.\n",
      "Found 16335 images belonging to 2 classes.\n",
      "Found 3223 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_height, img_width = 64, 64\n",
    "batch_size = 32\n",
    "\n",
    "# One datagen for both training and validation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Training generator (80%)\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    '../Datasets/New_Eyes/data/train',\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Validation generator (20%)\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    '../Datasets/New_Eyes/data/train',\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Test generator (separate test set)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    '../Datasets/New_Eyes/data/test',\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693ff003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 71ms/step - accuracy: 0.9442 - loss: 0.1593 - val_accuracy: 0.8877 - val_loss: 0.2892\n",
      "Epoch 2/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 38ms/step - accuracy: 0.9586 - loss: 0.1148 - val_accuracy: 0.8986 - val_loss: 0.2597\n",
      "Epoch 3/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 37ms/step - accuracy: 0.9673 - loss: 0.0929 - val_accuracy: 0.8984 - val_loss: 0.2671\n",
      "Epoch 4/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 38ms/step - accuracy: 0.9717 - loss: 0.0828 - val_accuracy: 0.9109 - val_loss: 0.2317\n",
      "Epoch 5/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 38ms/step - accuracy: 0.9759 - loss: 0.0709 - val_accuracy: 0.8919 - val_loss: 0.2679\n",
      "Epoch 6/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 38ms/step - accuracy: 0.9763 - loss: 0.0678 - val_accuracy: 0.9055 - val_loss: 0.2379\n",
      "Epoch 7/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 38ms/step - accuracy: 0.9774 - loss: 0.0632 - val_accuracy: 0.9194 - val_loss: 0.2100\n",
      "Epoch 8/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 38ms/step - accuracy: 0.9793 - loss: 0.0587 - val_accuracy: 0.9170 - val_loss: 0.2206\n",
      "Epoch 9/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 38ms/step - accuracy: 0.9802 - loss: 0.0563 - val_accuracy: 0.9092 - val_loss: 0.2237\n",
      "Epoch 10/10\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 38ms/step - accuracy: 0.9816 - loss: 0.0535 - val_accuracy: 0.9101 - val_loss: 0.2314\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,\n",
    "    validation_data=val_gen,  # <- CV set used here\n",
    "    callbacks=[],  # (Optional) Add EarlyStopping or ReduceLROnPlateau if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a35174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 194ms/step - accuracy: 0.9587 - loss: 0.1393\n",
      "Test Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_gen)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40396ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"models/open_close_eyes_model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e30dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load your trained model ===\n",
    "# model = load_model(\"models/open_close_eyes_model2.h5\")  # Replace with actual path\n",
    "\n",
    "img_size = (64, 64)  # Model input size\n",
    "\n",
    "# === Load Haar cascades ===\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# === Read image ===\n",
    "img = cv2.imread(\"../Datasets/o1.jpg\")  # Replace with your image path\n",
    "\n",
    "# === Resize image if too large ===\n",
    "max_width = 800\n",
    "max_height = 600\n",
    "height, width = img.shape[:2]\n",
    "if width > max_width or height > max_height:\n",
    "    scale = min(max_width / width, max_height / height)\n",
    "    img = cv2.resize(img, (int(width * scale), int(height * scale)))\n",
    "\n",
    "# === Convert to grayscale ===\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# === Detect faces ===\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "    # === Detect eyes in face ROI ===\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex, ey, ew, eh) in eyes:\n",
    "        # Eye region must be in upper half of face\n",
    "        if ey > h // 2:\n",
    "            continue\n",
    "\n",
    "        # Optional: filter eyes based on size\n",
    "        if ew < 10 or eh < 10 or ew > w // 2:\n",
    "            continue\n",
    "\n",
    "        eye_gray = roi_gray[ey:ey+eh, ex:ex+ew]\n",
    "        eye_resized = cv2.resize(eye_gray, img_size)  # Resize to 64x64\n",
    "        eye_array = eye_resized.astype(\"float32\") / 255.0  # Normalize\n",
    "        eye_array = np.expand_dims(eye_array, axis=-1)  # Add channel dim\n",
    "        eye_array = np.expand_dims(eye_array, axis=0)   # Add batch dim\n",
    "\n",
    "        # === Predict ===\n",
    "        prediction = model.predict(eye_array)[0][0]\n",
    "        label = \"Open\" if prediction >= 0.5 else \"Closed\"\n",
    "\n",
    "        # === Draw results ===\n",
    "        cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "        cv2.putText(roi_color, label, (ex, ey - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6, (255, 255, 255), 2)\n",
    "\n",
    "# === Display final result ===\n",
    "cv2.imshow(\"Eye Detection & Classification\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e298db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Load trained model ===\n",
    "model = load_model(\"models/open_close_eyes_model2.h5\")  # Replace with your model path\n",
    "img_size = (64, 64)\n",
    "\n",
    "# === Initialize MediaPipe face mesh ===\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "# === Load Haar cascades ===\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# === Read image ===\n",
    "img = cv2.imread(\"../Datasets/n3.jpg\")  # Replace with your image path\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not found!\")\n",
    "\n",
    "# === Resize image if too large ===\n",
    "max_width, max_height = 800, 600\n",
    "h, w = img.shape[:2]\n",
    "if w > max_width or h > max_height:\n",
    "    scale = min(max_width / w, max_height / h)\n",
    "    img = cv2.resize(img, (int(w * scale), int(h * scale)))\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "# === Convert BGR to RGB for MediaPipe ===\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "results = face_mesh.process(rgb_img)\n",
    "\n",
    "# === Flag to check if any eye was processed ===\n",
    "eye_detected = False\n",
    "\n",
    "# === Try MediaPipe ===\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        left_eye_indices = [33, 133]   # Outer corners of left eye\n",
    "        right_eye_indices = [362, 263] # Outer corners of right eye\n",
    "\n",
    "        def extract_eye_region(indices):\n",
    "            x_coords = [int(face_landmarks.landmark[i].x * w) for i in indices]\n",
    "            y_coords = [int(face_landmarks.landmark[i].y * h) for i in indices]\n",
    "            x_min, x_max = min(x_coords), max(x_coords)\n",
    "            y_min, y_max = min(y_coords), max(y_coords)\n",
    "            margin_x = int((x_max - x_min) * 0.4)\n",
    "            margin_y = int((y_max - y_min) * 1.2)\n",
    "            x1 = max(x_min - margin_x, 0)\n",
    "            y1 = max(y_min - margin_y, 0)\n",
    "            x2 = min(x_max + margin_x, w)\n",
    "            y2 = min(y_max + margin_y, h)\n",
    "            return x1, y1, x2, y2\n",
    "\n",
    "        for eye_label, indices in zip([\"Left\", \"Right\"], [left_eye_indices, right_eye_indices]):\n",
    "            x1, y1, x2, y2 = extract_eye_region(indices)\n",
    "            eye_roi = img[y1:y2, x1:x2]\n",
    "            if eye_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            eye_gray = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2GRAY)\n",
    "            eye_resized = cv2.resize(eye_gray, img_size)\n",
    "            eye_array = eye_resized.astype(\"float32\") / 255.0\n",
    "            eye_array = np.expand_dims(eye_array, axis=[0, -1])\n",
    "\n",
    "            prediction = model.predict(eye_array)[0][0]\n",
    "            label = \"Open\" if prediction >= 0.5 else \"Closed\"\n",
    "\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\" {label}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "            eye_detected = True\n",
    "\n",
    "# === If no eyes detected by MediaPipe, fallback to Haar ===\n",
    "if not eye_detected:\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            if ey > h // 2 or ew < 10 or eh < 10 or ew > w // 2:\n",
    "                continue\n",
    "\n",
    "            eye_gray = roi_gray[ey:ey+eh, ex:ex+ew]\n",
    "            eye_resized = cv2.resize(eye_gray, img_size)\n",
    "            eye_array = eye_resized.astype(\"float32\") / 255.0\n",
    "            eye_array = np.expand_dims(eye_array, axis=[0, -1])\n",
    "\n",
    "            prediction = model.predict(eye_array)[0][0]\n",
    "            label = \"Open\" if prediction >= 0.5 else \"Closed\"\n",
    "\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "            cv2.putText(roi_color, label, (ex, ey - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "# === Show final result ===\n",
    "cv2.imshow(\"Eye Detection & Classification (Fallback Ready)\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
